\newpage
\lecture{3}{Сингулярное разложение.}

\subsection{Нормированные пространства.}

\begin{definition}
    \mdef{Нормированным векторным пространством} называется векторное пространство с заданной на нем нормой.
\end{definition}

\begin{exercise}
    Пусть $x\in\R^n,\ x = (x_1,\, x_2,\, \ldots,\, x_n)$.
    Различные нормы:
    \begin{enumerate}
        \item $\|x\|_2:=\sqrt{\sum\limits_{i=1}^n |x_i|^2}$ ~--- 2-норма.
        \item $\|x\|_p:=\left(\sum\limits_{i=1}^n|x_i|^p\right)^{1/p}$~--- $p$"=норма Гёльдера, где $p\geqslant 1$.
    \end{enumerate}
\end{exercise}

А как определить норму для матриц $A\in\R^{m\times n}$? Есть несколько подходов:
\begin{enumerate}
    \item <<Вытянуть>> матрицу в длинный вектор и взять одну из норм выше (или какую-то другую норму для векторов из $\R^n$).
          Например, \mdef{норма Фробениуса}~--- 2-норма на получившемся векторе (обозначается $\|A\|_F$).
    \item Операторная норма (смотри определение ниже).
\end{enumerate}

\begin{definition}
    \mdef{Операторная норма} матрицы $A$:
    \[
        \|A\|_p:=\sup_{\vect{x}\neq 0}\dfrac{\|A\vect{x}\|_p}{\|\vect{x}\|_p}
    \]
\end{definition}

\begin{exercise}
    Докажите, что норма Фробениуса не является операторной нормой.
\end{exercise}

\subsection{Задача о приближении.}
Допустим мы хотим приблизить матрицу $A$ матрицей $\widetilde{A}$ так, чтобы матрица $\widetilde{A}$ обладала некоторыми
полезными свойствами, чтобы с ней было проще работать и при этом она не сильно отличалась от матрицы $A$ в терминах нормы.

Пусть $A=[a_{ij}]$. Понятно, что коэффициенты $a_{ij}$ можно рассматривать как некоторую функции от дискретных аргументов:
$a_{ij}=a(i,\,j)$, тогда задача сводится к нахождению близкой функции $\widetilde{a}(i,\, j)\approx a(i,\, j)$.

\begin{definition}
    Если для функции $f(x,\, y)$ выполнено, что $f(x,\, y)=u(x)\cdot v(y)$, то говорят, что $f$~--- функция с \mdef{разделенными переменными}
    (другое название \mdef{<<чистая функция>>}).
\end{definition}

Можно разложить функцию $f$ в сумму чистых функций:
\[
    f(x,\,y)\approx \sum_{\alpha=1}^{r} u_{\alpha}(x)v_{\alpha}(y).
\]

\begin{claim}
    Любая функция дискретных переменных представима в виде конечной суммы чистых функций.

    \begin{proof}
        Приведем разложение в сумму чистых функций нужной нам функции (и тем самым и докажем утверждение в общем случае), а именно
        $a(i,\, j)$. Понятно, что
        \[
            a_{i_0j_0}=\sum_{j=1}^{m}\sum_{j=1}^n(a_{ij}\delta_{ii_0})\delta_{jj_0}\text{~--- сумма чистых функций,}
        \]
        где $\delta_{ij}$~--- символ Кронекера.

    \end{proof}
\end{claim}

\begin{remark}
    Для функций большего числа аргументов чистота естественно обобщается. Более того сформулированное утверждение остается верным.
\end{remark}

Можно от функций перейти сразу к матрицам. Итак, пусть
\[
    [u(i)v(j)]_{m\times n} = \left[
        \begin{array}{c}
            u(1)   \\
            \ldots \\
            u(m)
        \end{array}
        \right] \left[v(1)\ \ldots\ v(n)\right].
\]

По аналогии с утверждением про функции, верно и утверждение про матрицы:

\begin{claim}
    Любая матрица является суммой матриц вида <<столбец$\times$строка>> (чистых матриц). Тогда можно ввести понятие минимального представления
    матрицы в виде суммы чистых матриц (представление, в котором число слагаемых минимально).
\end{claim}

А теперь задумаемся, а каково минимальное число слагаемых в таком разложении? Ответ в следующем красивом утверждении:
\begin{claim}
    Минимальное число чистых матриц, дающих в сумме заданную матрицу, равно её рангу.
    Данное утверждение можно принять за одно из определений ранга матрицы.
\end{claim}

\begin{exercise}
    Рассмотрим случай трехмерной матрицы. Пусть $A = [a_{ijk}]$. Тогда можно записать
    \[
        a_{ijk}=\sum_{\alpha=1}^r u_{i\alpha}v_{j\alpha}w_{k\alpha},
    \]
    где $\min r$~--- тензорный ранг трёхмерной матрицы. И аналогично для больших размерностей.
\end{exercise}

Вернёмся к исходной задаче. Уточним ее немного, пусть $A\approx \widetilde{A}$, где
$\rk(\widetilde{A})\leqslant k<\rk(A)$, то есть можем приблизить $A$ близкой матрицей, с небольшим рангом, то есть
представимой в виде суммы небольшого числа чистых матриц.

Некоторые применения данного приближения:
\begin{enumerate}[label=\protect\circled{\arabic*}]
    \item Сжатие информации. В самом деле, матрицы размером $n\times m$ можно записывать в виде суммы
          $r$ слагаемых размером $m+n$. Видно, что $r(n+m)\ll nm$.
    \item Борьба с <<шумом>>. При замене матрицы на матрицу меньшего ранга, часто случается, что теряем мы как раз
          ненужную нам информацию, так называемый \textit{шум}.
    \item Построение более эффективных алгоритмов за счёт перехода к матрицам меньшего ранга.
\end{enumerate}

\subsection{Сингулярное разложение.}
Матрицу $A\in\R^{n\times m}$ можно рассматривать как линейный оператор $A: \R^n\rightarrow \R^m$ такой, что
$\vect{x}\in\R^n\rightarrow A\vect{x}\in\R^m$.

\begin{theorem}
    В пространствах $\R^n$ и $\R^m$ можно выбрать ортонормированный базис, в которых оператор представляется диагональной
    матрицей следующего вида:
    \[
        \Sigma = \left(
        \begin{array}{c|c}
                \Sigma_r & O \\
                \hline
                O        & O \\
            \end{array}
        \right),\quad
        \Sigma_r=
        \left(
        \begin{array}{cccc}
                \sigma_1 & 0        & \ldots & 0        \\
                0        & \sigma_2 & \ldots & 0        \\
                \ldots                                  \\
                0        & 0        & \ldots & \sigma_r \\
            \end{array}
        \right), \quad \sigma_i>0\ \forall i=\overline{1..r}.
    \]
\end{theorem}

\begin{next0}
    Для любой вещественной матрицы $A$ существуют ортогональные матрицы $U\in\R^{n\times n}$ и $V\in\R^{m\times m}$
    такие, что
    \[
        A=V\Sigma U^T.
    \]
\end{next0}

\begin{remark}
    Для комплексных матриц данное утверждение также верно, но вместо транспонирования используется эрмитово
    сопряжение\footnote{транспонирование и замена каждого элемента на сопряженный ему, обозначается $U^*$.}.
\end{remark}

\begin{definition}
    \mdef{Унитаарная матрица}~--- квадратная матрица с комплексными элементами,
    результат умножения которой на эрмитово сопряжённую равен единичной матрице:
    \[
        A\text{~--- унитарная} \Longleftrightarrow A^*A=AA^*=I.
    \]
\end{definition}

Благодаря этому следствию, можно ввести следующее понятие:

\begin{definition}
    \mdef{Сингулярное разложение} матрицы $A$:
    \[
        A=V\Sigma U^*,
    \] где $V,\, U$~--- унитарные, $\Sigma$~--- определена выше.
\end{definition}

Обобщим следствие в виде теоремы:
\begin{theorem}
    Для любой комплексной матрицы $A$ существует сингулярное разложение.
\end{theorem}

\begin{definition}
    Матрица $A$ называется \mdef{эрмитовой}, если $A^*=A$.
\end{definition}

\begin{theorem}
    Для любой эрмитовой матрицы $A$ существует унитарная матрица $Q$ такая, что
    \[
        Q^*AQ=\Lambda\text{~--- диагональная и вещественная.}
    \]

    Говорят, что такие матрицы \mdef{унитарно конгруэнтны} диагональным матрицам. Преобразование
    $A\rightarrow Q^*AQ$ называется \mdef{конгруэнцией} или \mdef{конгруэнтным преобразованием}.
\end{theorem}

Рассмотрим теорему поближе. Равенство $Q^*AQ=\Lambda$ можно переписать так: $AQ=Q\Lambda$. Пусть
$Q=[\vect{q_1},\, \vect{q_2},\,\ldots,\,\vect{q_n}]$, тогда
\[
    AQ=Q\Lambda\Longleftrightarrow \begin{cases}
        A\vect{q_1}=\lambda_1\vect{q_1}, \\
        A\vect{q_2}=\lambda_2\vect{q_2}, \\
        \ldots                           \\
        A\vect{q_n}=\lambda_n\vect{q_n}. \\
    \end{cases},
\]
где $\lambda_i$~--- $i$"=ое число на диагонали матрицы $\Lambda$.

Видно, что в терминах собственных векторов и собственных значений равенства означают, что $\vect{q_i}$
является собственным вектором собственного значения $\lambda_i$.

То есть теорему можно переформулировать так: \textit{любая эрмитова матрица обладает ортонормированным
    базисом из своих собственных векторов}.

\begin{remark}
    Но есть и другие матрицы, обладающие таким свойствам. Все такие матрицы называются
    \mdef{нормальными}. Но удивительно то, что определение нормальных матриц немного другое (см. ниже).
\end{remark}

\begin{definition}
    Матрица $A$ называется нормальной, если $A^*A=AA^*$.
\end{definition}

Проверим, также что $\Lambda$~--- действительно вещественная. В самом деле,
$A=A^*,\, \Lambda=Q^*AQ\Rightarrow \Lambda^*=Q^*A^*Q=Q^*AQ=\Lambda$, но $\Lambda$~--- диагональная, а следовательно
$\lambda_i=\overline{\lambda_i}\Rightarrow\lambda_i\in\R$, что и требовалось показать.

\subsection{Получение сингулярного разложения.}
Пусть $A\in\Cx^{m\times n}$, тогда $A^*A\in\Cx^{n\times n}$~--- эрмитова. В самом деле,
$(A^*A)^*=A^*A$, следовательно она обладает ортонормированным базисом из собственных векторов.
Пусть $\vect{u_1},\, \vect{u_2},\,\ldots,\, \vect{u_n}$~--- этот самый базис.
То есть $A^*A\vect{u_i}=\lambda_i\vect{u_i}$, причем $\lambda_i\geqslant 0$. В самом деле,
\[
    A^*A\vect{u_i}=\lambda_i\vect{u_i}\Rightarrow
    \vect{u_i}^*A^*A\vect{u_i}=\vect{u_i}^*\lambda_i\vect{u_i}\Rightarrow
    (A\vect{u_i},\, A\vect{u_i})=\lambda_i(\vect{u_i},\, \vect{u_i})\Rightarrow
    \lambda_i\geqslant 0.
\]

Поэтому без ограничения общности можем положить, что нумерация векторов в базисе выбрана таким образом,
что вначале идут векторы, соответствующие положительным $\lambda$, а после нулевым:

\[
    A^*A\vect{u_i}=\begin{cases}
        \sigma_{i}^2\vect{u_i},\, & 1\leqslant i\leqslant r,   \\
        0,\,                      & r+1\leqslant i\leqslant n,
    \end{cases}
\]
где $\sigma_i$~--- \mdef{сингулярные числа}.

Введем векторы
\[
    \vect{v_i}:=\dfrac{A\vect{u_i}}{\sigma_i},\quad 1\leqslant i\leqslant r.
\]

Также векторы $\vect{u_i},\, \vect{v_i}$~--- \mdef{сингулярные векторы}.

Заметим, что новые векторы $\vect{v_1},\,\ldots,\, \vect{v_r}$~--- ортонормированные:
\[
    (\vect{v_i},\, \vect{v_j}) = \left(\dfrac{A\vect{u_i}}{\sigma_i},\, \dfrac{A\vect{u_j}}{\sigma_j}\right)=
    \dfrac{1}{\sigma_i\sigma_j}(A^*A\vect{u_i},\, \vect{u_j})=\dfrac{\sigma_i^2}{\sigma_i\sigma_j}(\vect{u_i},\,
    \vect{u_j})=\begin{cases}
        1, & i=j,     \\
        0, & i\neq j.
    \end{cases}
\]

Далее любую ортонормированную систему можно дополнить до ортонормированного базиса:
$\vect{v_1},\,\ldots,\,\vect{v_r},\,\vect{v_{r+1}},\,\ldots,\,\vect{v_m}$~--- ортонормированный базис
пространства $\Cx^m$. Остается посмотреть на значение $A\vect{u_i}$:
\[
    A\vect{u_i}=\begin{cases}
        \sigma_i\vect{v_i}, & 1\leqslant i\leqslant r,   \\
        0,                  & r+1\leqslant i\leqslant n.
    \end{cases}
\]

В самом деле, $\vect{u}^*A^*A\vect{u}=\vect{u}^*\cdot 0\Rightarrow (A\vect{u},\,A\vect{u})=0\Rightarrow
    A\vect{u}=0$. И вспоминаем, что эти равенства эквивалентны уравнению $AU=V\Sigma$, где $\Sigma$~--- та самая матрица из
сигм. Понятно, что действующее выше $r$ нечто иное как ранг матрицы $A$.

Заметим еще одно свойство (следует из равенства $A=V\Sigma U^*$ после взятия сопряжения):
\[
    A^*\vect{v_i}=\begin{cases}
        \sigma_i\vect{u_i}, & 1\leqslant i\leqslant r,   \\
        0,                  & r+1\leqslant i\leqslant n.
    \end{cases}
\]

Далее, вспомним про ядро и образ преобразования:

\begin{align*}
     & \ker (A)=L(\vect{u_{r+1}},\, \ldots,\, \vect{u_n}),\quad \im(A)=L(\vect{v_1},\,\ldots,\,\vect{v_r}),     \\
     & \ker (A^*)=L(\vect{v_{r+1}},\, \ldots,\, \vect{v_n}),\quad \im(A^*)=L(\vect{u_1},\,\ldots,\,\vect{u_r}).
\end{align*}

Из этих равенств следует, что
\begin{align*}
    \Cx^n & =\ker(A)\oplus\im(A^*), \\
    \Cx^m & =\ker(A^*)\oplus\im(A).
\end{align*}


\begin{remark}
    Из последнего равенства следует \mdef{альтернатива Фредгольма}:
    Либо система $Ax=b$ имеет решение при любой правой части, либо система $A^*u=0$ имеет нетривиальное решение.    
\end{remark}


\subsection{Связь с задачей приближения.}

\begin{theorem}
    \label{lect3:last}
    Пусть $A=V\Sigma U^*=\sum\limits_{i=1}^r\sigma_i\vect{v_i}\vect{u_i}^*$. Обозначим 
    $A_k:=\sum\limits_{i=1}^k\sigma_i\vect{v_i}\vect{u_i}^*$ в предположении, что 
    $\sigma_1\geqslant \sigma_2\geqslant\ldots\geqslant \sigma_r>0$.
    Тогда \[
        \min_{\rk(B)\leqslant k} \|A-B\|_2=\|A-A_k\|_2=\sigma_{k+1}.
    \]
\end{theorem}

\begin{remark}
    \mdef{2-норма} матрицы $A$ также называется \mdef{спектральной нормой}.
\end{remark}

\begin{claim}
    $\|A\|_2=\sigma_{\max}(A)$.

    \begin{proof}
        На прошлой лекции обсуждалось, что при умножении на унитарную матрицу длина вектора не меняется, используя этот 
        факт имеем:
        \[
            \dfrac{\|A\vect{x}\|}{\|\vect{x}\|}=\dfrac{\|V\Sigma U^*\vect{x}\|}{\|\vect{x}\|}=
            \dfrac{\|\Sigma U^*\vect{x}\|}{\|U^*\vect{x}\|}=\dfrac{\|\Sigma\vect{y}\|}{\|\vect{y}\|}, \quad \vect{y}=U^*x\vect{x}.
        \]

    \end{proof}
\end{claim}