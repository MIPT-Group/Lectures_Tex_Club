\newpage
\lecture{4}{Вычисление сингулярного разложения.}

\subsection{Продолжение теоремы о приближении.}

Вспомним прошлую лекцию, а именно сингулярное разложение.
Итак пусть матрица $A\in\Cx^{m\times n}$ представлена в виде: $A=V\Sigma U^*$, где $V,\, U$~--- унитарные матрицы, а
\[
    \Sigma = \left(
    \begin{array}{c|c}
            \Sigma_r & O \\
            \hline
            O        & O \\
        \end{array}
    \right),\quad
    \Sigma_r=
    \left(
    \begin{array}{cccc}
            \sigma_1 & 0        & \ldots & 0        \\
            0        & \sigma_2 & \ldots & 0        \\
            \ldots                                  \\
            0        & 0        & \ldots & \sigma_r \\
        \end{array}
    \right), \quad \sigma_1\geqslant \sigma_2\geqslant\ldots\geqslant\sigma_r\geqslant 0.
\]
Пусть матрицы $U$ и $V$ состоят из столбцов: $U=[\vect{u_1},\, \vect{u_2},\,\ldots,\, \vect{u_n}],\, V=[\vect{v_1},
            \,\vect{v_2},\,\ldots,\, \vect{v_m}]$.
Тогда
\[
    A=V\Sigma U^*\Leftrightarrow A\vect{u_i}=\begin{cases}
        \sigma_i\vect{v_i},\  & 1\leqslant i\leqslant r,   \\
        \vect{0},\            & r+1\leqslant i\leqslant n.
    \end{cases}\Leftrightarrow
    A^*\vect{v_i}=\begin{cases}
        \sigma_i\vect{u_i},\  & 1\leqslant i\leqslant r,   \\
        \vect{0},\            & r+1\leqslant i\leqslant n.
    \end{cases}
\]
Вспомним, что $\vect{u_i},\, \vect{v_i}$~--- сингулярные векторы, $\sigma_i$~--- сингулярные числа.
Матрицу $A$ можно представить еще и так:
\[
    A=\sum_{i=1}^r \sigma_i\vect{v_i}\vect{u_i^*}, \quad r = \rk A.
\]

Снова введем <<\textit{обрезанное}>> сингулярное разложение:
\[
    A_k:=\sum_{i=1}^k\sigma_i\vect{v_i}\vect{u_i^*}.
\]

В силу упорядоченности сингулярных чисел, отбрасываемая часть не сильно велика, поэтому можем ожидать, что $A_k\approx A$ (смотри теорему \ref{lect3:last}).
Более того, можно сформулировать следующее утверждение:

\begin{claim}
    \[
        \|A\|_2=\sigma_{max}(A).
    \]
    \
    \begin{proof}
        Умножение на унитарные матрицы не меняет норму матрицы:
        \[
            \|\underbrace{V^*AU}_{\Sigma}\|_2=\|A\|_2=
            \sup_{\vect{x}\neq \vect{0}}\dfrac{\|\Sigma \vect{x}\|_2}{\|\vect{x}\|_2}=\sup_{\vect{x}\neq\vect{0}}
            \dfrac{\sqrt{\sum_{i = 1}^r\sigma_i^2|x_i|^2}}{\sqrt{\sum_{i=1}^n |x_i|^2}}\leqslant
            \sigma_1.
        \]
        Теперь если возьмем $\vect{x^*}=(1,\, 0,\, \ldots,\, 0)$, то получаем, что $\|A\|_2\geqslant\sigma_1$. Неравенство доказано в обе стороны,
        что и требовалось.

    \end{proof}
\end{claim}

\begin{remark}
    Рассмотрим матрицу $A^*A$:
    \[
        A=V\Sigma U^*\Rightarrow A^*A=U\Sigma^*V^*V\Sigma U^*=
        U\Sigma^*\Sigma U^*=U\Sigma^*\Sigma U^{-1},
    \]
    то есть матрица $A^*A$ подобна матрице $\Sigma^*\Sigma$, а эта матрица отличается от $\Sigma$ только тем, что
    вместо сингулярных чисел стоят их квадраты, то есть $\sigma_i^2$~--- это сингулярные числа матрицы $A^* A$.
\end{remark}

Докажем теперь теорему с прошлой лекции. Повторим её.
\begin{theorem}
    \label{lect4:prev}
    Пусть $A=V\Sigma U^*=\sum\limits_{i=1}^r\sigma_i\vect{v_i}\vect{u_i}^*$. Обозначим
    $A_k:=\sum\limits_{i=1}^k\sigma_i\vect{v_i}\vect{u_i}^*$ в предположении, что
    $\sigma_1\geqslant \sigma_2\geqslant\ldots\geqslant \sigma_r>0$.
    Тогда \[
        \min_{\rk(B)\leqslant k} \|A-B\|_2=\|A-A_k\|_2=\sigma_{k+1}.
    \]

    \begin{proof}
        Возьмем матрицу $B$ такую, что $\rk(B)\leqslant k$. Тогда
        \begin{equation}
            \label{eq:lect4:1}
            \|A-B\|_2=\max_{\|\vect{x}\|_2=1}\|(A-B)\vect{x}\|_2\geqslant\max_{\|\vect{x}\|_2=1,\,B\vect{x}=0}\|(A-B)\vect{x}\|_2=
            \max_{\|\vect{x}\|_2=1,\,B\vect{x}=0}\|A\vect{x}\|_2.
        \end{equation}
        Множество $\{\vect{x}\ \vert\ B\vect{x}=0\}$ называется \mdef{ядром} матрицы $B$. Обозначение: $\ker(B)$. Легко понять, что
        ядро является пространством, и его размерность равна $\dim \ker (B)=n-\rk(B)\geqslant n-k$.

        Далее возьмем подпространство $L=L(\vect{u_1},\,\ldots,\, \vect{u_{k+1}})$. Тогда $\dim L = k + 1$, то есть
        $\dim \ker(B)+\dim L\geqslant n+1\Rightarrow\exists \vect{x}:\ \|\vect{z}\|_2=1$ такой, что $\vect{z}\in\ker(B)\cap L$.
        А значит, возвращаясь к неравенствам в уравнении \eqref{eq:lect4:1}, получаем:
        \[
            \|A-B\|_2\geqslant \max_{\|\vect{x}\|_2=1,\, B\vect{x}=0}\|A\vect{x}\|_2\geqslant \|A\vect{z}\|_2\geqslant
            \min_{\vect{x}\in L,\, \|\vect{x}\|_2=1}\|A\vect{x}\|_2.
        \]
        Найдем значение последнего минимума:
        \[
            \vect{x}\in L\Rightarrow \vect{x}=\sum_{i=1}^{k+1}\xi_iu_i\Rightarrow A\vect{x}=\sum_{i=1}^{k+1}\sigma_i\xi_i\vect{v_i}\Rightarrow
            \|A\vect{x}\|_2=\sqrt{\sum_{i=1}^{k+1}\sigma_i^2|\xi_i|^2}\geqslant \sigma_{k+1}\underbrace{\sqrt{\sum_{i=1}^{k+1}|\xi_i|^2}}_{1}.
        \]

        Итак, осталось доказать, что этот минимум достигается. Но этот шаг довольно простой, возьмем $B=A_k$, тогда для матрицы $A-A_k$
        старшее сингулярное число и есть $\sigma_{k+1}$, то есть минимум достигается, что и требовалось.

    \end{proof}
\end{theorem}

\subsection{Минутка практического применения.}

\begin{exercise}
    При поиске чего-либо в интернете мы сталкиваемся с задачами, которые уже были рассмотрены.
    При поиске страницы (документа) по ключевым словам используется \mdef{частотная матрица}:
    матрица $A$, в которой на позиции $(i,\, j)$~--- частота встречаемости ключевого слова $j$ в документе $i$.
    Во-первых, такая матрица невероятно огромная, и возникает проблема хранить эту матрицу. Более того, нужно еще
    и искать в ней нужные строчки. Можно искать например так, пусть $\vect{c}$~--- вектор интересующих нас ключевых слов.
    Тогда нужно найти документы $d$ такие, что значение $(\vect{a_{d}},\, \vect{c})$ максимально ($\vect{a_{d}}$~--- $d$"=ая строка в матрице $A$).
    Так же можно брать ключевые слова с разными весами (это и понятно, есть слова которые более релевантны к предмету поиска, а есть которые менее).

    Конечно же, во-первых, это огромная вычислительная сложность, во-вторых, в матрице может быть какой-нибудь шум, да и пользователь может не точно сформулировать запрос.
    Поэтому возникает идея, рассмотреть сингулярное разложение матрицы $A$: $A\approx VU^T$. Тогда $\vect{a_i}=\vect{v_i}U^T$. И теперь скалярные произведения
    считать проще: $(\vect{a_i},\, c)=\vect{v_i}$\circled{$U^t\vect{c}$}. Нужно найти вектор в кружке~--- это достаточно быстро, так как в матрице $U$ не так много строк,
    если взять к примеру $k$"=ранговое приближение порядка десятка.
\end{exercise}

\subsection{Способы вычисления сингулярного разложения.}

\subsubsection{Алгоритм Голуба-Кахана.}

\begin{definition}
    Матрица, все ненулевые элементы которой находятся на главной диагонали и на одной из диагоналей над (или под, но не вместе) главной, называется
    \mdef{бидиагональной}.
\end{definition}

Алгоритм состоит из нескольких частей:
\begin{enumerate}[label=\protect\circled{\arabic*}]
    \item Заметим, что любую матрицу с помощью элементарных унитарных\footnote{преобразование умножением на матрицу вращения, смотри лекцию 2.}
          преобразование справа и слева можно привести к бидиагональному виду. То есть преобразовываем матрицу $A\rightarrow PAQ=D$~--- бидиагональная и
          $P,\, Q$~--- унитарные (так как являются матрицами поворота). Если найдем сингулярное разложение для $D$, то с лёгкостью найдем и для $A$. Более того
          сингулярные числа матриц $A$ и $D$ совпадают, поэтому если задача лишь о нахождении сингулярных чисел, то на этом шаге можно было бы уже остановиться.

    \item Теперь пусть $A$~--- бидиагональная, то есть
          \[
              A = \left[
                  \begin{array}{ccccc}
                      a_1^0 & b_1^0 & 0     & \ldots & 0     \\
                      0     & a_2^0 & b_2^0 & \ldots & 0     \\
                      0     & 0     & a_3^0 & \ldots & 0     \\
                      \ldots                                 \\
                      0     & 0     & 0     & \ldots & a_n^0
                  \end{array}
                  \right]
          \]
          Найдем для $A$ сингулярные числа.
          Пусть $A_0=A$. Построим $A_1$ из $A_0$ посредством вращения, такого, что $b_i^0$ обращается в нуль. Но тогда $A_1$ уже будет нижней бидиагональной:
          \[
              A_1 = \left[
                  \begin{array}{ccccc}
                      a_1^1 & 0     & 0     & \ldots & 0     \\
                      b_1^1 & a_2^1 & 0     & \ldots & 0     \\
                      0     & b_2^1 & a_3^1 & \ldots & 0     \\
                      \ldots                                 \\
                      0     & 0     & 0     & \ldots & a_n^1
                  \end{array}
                  \right]
          \]

          Возникает вопрос: а что изменилось? Мы на него не отвечаем и продолжаем процесс. Строим теперь $A_2$, обнуляя $b_i^1$.
          Возникнет последовательность:
          \[
              A_0\rightarrow A_1\rightarrow A_2\rightarrow A_3\rightarrow\ldots,
          \]
          где $A_{2k}$~--- верхняя бидиагональная, а $A_{2k+1}$~--- нижняя бидиагональная.

          Вопрос выше всплывает снова. Но оказывается, что в такой последовательности недиагональные элементы стремятся к нулю.
          И ответ на вопрос становится очевидным, в тот момент, когда недиагональные коэффициенты для нас уже достаточно малы, можем их полностью занулить
          и перейти к новой, уже диагональной матрице, которая близка к исходной. Поясним кратко, почему же они стремятся к нулю.
          Рассмотрим переход $A_0\rightarrow A_1$. Это ничто иное, как умножение $A_0$ справа на некоторую унитарную матрицу, то есть длина каждой
          строки не поменялась. То есть можем записать:
          \[
              \begin{cases}
                  (a_1^0)^2+(b_1^0)^2 =(a_1^1)^2             \\
                  (a_2^0)^2+(b_2^0)^2 =(b_1^1)^2 + (a_2^1)^2 \\
                  \ldots                                     \\
              \end{cases}
          \]

          Аналогично, как связаны $A_1$ и $A_2$? Теперь длины столбцов равны, то есть получаются такие же уравнения. Обобщая можно написать:
          для $l$"=ой матрицы:
          \[
              \begin{cases}
                  (a_1^l)^2+(b_1^l)^2 =(a_1^{l+1})^2                 \\
                  (a_2^l)^2+(b_2^l)^2 =(b_1^{l+1})^2 + (a_2^{l+1})^2 \\
                  \ldots                                             \\
              \end{cases}
          \]

          Из этих равенств можно заметить, что $b_i^n\xrightarrow[n\rightarrow\infty]{}0$.

\end{enumerate}

\subsubsection{Анализ алгоритма. Скорость сходимости.}

Зададимся вопросом, а насколько быстро сходятся к нулю коэффициенты? Пусть $\sigma_1>\sigma_2>\ldots>$~--- сингулярные числа.
Тогда скорость убывания элемента в позиции $(i,\, i + 1)$ в $l$"=ой матрице задается выражением:
\[
    \left(\dfrac{\sigma_{i}}{\sigma_{i+1}}\right)^l.
\]
Идейно доказывается этот факт так: при умножении на унитарную сохраняются еще и углы между строками, рассмотрев их и равенства выше
можно получить данный результат.

Теперь рассмотрим число операций.
\begin{enumerate}[label=\protect\circled{\arabic*}]
    \item Здесь всё просто, операций $O(n^3)$, где $n$~--- порядок матрицы (анализ проводился в прошлых лекциях).
    \item Здесь для обнуления одного элемента нужно затратить $O(1)$ арифметических действий, тогда всего будет $O(n)$ арифметических операций
          на одну итерацию. Тогда итого этот шаг занимает $O(n)\cdot \#\text{итераций}$ действий.
\end{enumerate}

\begin{remark}
    Тем не менее у алгоритма есть пара недостатков:
    \begin{enumerate}
        \item Если сингулярные числа близки, то скорость сходимости мала, и выигрыш по числу операций сводится на нет.
        \item В алгоритме никак не учитываются особенности матрицы~--- он одинаково работает и для разреженных матриц, и для достаточно заполненных, и
              для матриц специального вида.
    \end{enumerate}
\end{remark}

