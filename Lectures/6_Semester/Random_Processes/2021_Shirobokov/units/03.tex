\newpage
\lecture{3}{Нормальные процессы, винеровский процесс}
\subsection{Обобщения пуассоновского процесса}
Пуассоновский процесс можно записать как
\begin{align*}
  & K(t) = \sum_{i=1}^{K(t)} 1
\end{align*}
\begin{Def}
    Процесс
    \begin{align*}
      & K_C(t) = \sum_{i=1}^{K(t)} \xi_i
    \end{align*}
    где $\xi_i$ независимы в совокупности и от $K(t)$, называется \textbf{сложным пуассоновским процессом (compound Poisson process)}.
\end{Def}
\begin{Prop}
    Сложный пуассоновский процесс, где $\xi_i \in Be(p)$, будет пуассоновским процессом с интенсивностью $\lambda p$.
\end{Prop}
\begin{Def}
    Если в определении пуассоновского процесса заменить третий пункт на
    \begin{align*}
      & K(t) - K(s) \in Po\left(\int_t^s \lambda(\tau) d\tau\right)
    \end{align*}
    то это называется \textbf{неоднородным пуассоновским процессом}.
\end{Def}
\section{Нормальные (гауссовские) процессы}
\subsection{Нормальные (гауссовские) процессы}
\begin{Def}
    Случайный процесс называется \textbf{нормальным (гауссовским) процессом}, если все его конечномерные распределения гауссовские, то есть
    \begin{align*}
      & \forall n \in \NN \ \left(X(t_1), \dots, X(t_n)\right)
    \end{align*}
    имеют нормальное распределение.
\end{Def}
\begin{Def}
    Cлучайный вектор
    \begin{align*}
      & \forall n \in \NN \ \left(X(t_1), \dots, X(t_n)\right)
    \end{align*}
    \textbf{имеет нормальное распределение}, если
    \begin{align*}
      & \varphi_X(s) = \exp\left(i\mu^T s - \frac{1}{2}s^TRs\right), \ s \in \RR^n
    \end{align*}
    \begin{align*}
      & \mu = \EE X \in \RR^n, \ R = \EE\left( \cent{X}\cent{X}^T \right)
    \end{align*}
    Говорят, что
    \begin{align*}
      & X \in \cN(\mu, R)
    \end{align*}  
\end{Def}
\textbf{Свойства нормального вектора}
\begin{enumerate}
    \item Покомпонентно
    \begin{align*}
      & X \in \cN(\mu, R) \Rightarrow X_i \in \cN(\mu_i, R_{ii})
    \end{align*}
    А если все компоненты независимы, то верно и обратное.
    \item Если
    \begin{align*}
      & X \in \cN(\mu, R), \ \det R \neq 0
    \end{align*}
    то существует плотность распределения
    \begin{align*}
      & f(x) = \frac{1}{(2\pi)^{\frac{n}{2}}\sqrt{\det R}}\exp \left( -\frac{1}{2}(x-\mu)^TR^{-1}(x-\mu) \right), \ x \in \RR^n
    \end{align*}
    Если матрица вырождена, то плотности у вектора нет. В случае вырожденности
    есть линейно зависимые компоненты в векторе. Действительно, в этом случае
    \begin{align*}
      & \exists c \neq 0: Rc = 0;
    \end{align*}
    \begin{align*}
      & Rc = 0 \Rightarrow c^TRc = 0 \Rightarrow 0 = c^T \EE \cent{X}\cent{X}^Tc = \EE c^T \cent{X}\cent{X}^Tc = \EE\left( \cent{X}^Tc \right)^2 = 0 \Rightarrow \cent{X}^Tc = 0
    \end{align*}
    \item Пусть
    \begin{align*}
      & X \in \cN(\mu, R), \ X \in \RR^n
    \end{align*}
    Тогда
    \begin{align*}
      & \forall A \in \RR^{k\times n} AX \in \cN(A\mu, ARA^T)
    \end{align*}
    \item Вектор $X$ нормален тогда и только тогда, когда $\forall \{c_i\} \
    \sum c_ix_i$~--- нормальная (или константа, но константа~--- частный случай
    нормальной величины.)
    \item Компоненты нормального случайного вектора некоррелированы тогда и
    только тогда, когда независимы.
    \item Условные распределения подвекторов нормального вектора нормальны.
    \begin{align*}
      & X = \left( \xi, \eta \right) \in \cN(\mu, R), \ X \in \RR^{n+m}, \ \xi \in \RR^n, \ \eta \in \RR^m
    \end{align*}
    \begin{align*}
      & \mu = \left[ \begin{matrix}
              \mu_{\xi} \\
              \mu_{\eta}
          \end{matrix} \right], \ R = \left[ \begin{matrix}
              R_{\xi \xi} & R_{\xi \eta} \\
              R_{\eta \xi} & R_{\eta \eta}
          \end{matrix} \right]
    \end{align*}
    Пусть $\det R_{\eta \eta} \neq 0$, тогда
    \begin{align*}
      & \left( \xi \mid \eta = x \right) \in \cN\left( \mu_{\xi} + R_{\xi \eta}R^{-1}_{\eta \eta}(x-\mu\eta), R_{\xi \xi}-R_{\xi \eta}R_{\eta \eta}^{-1}R_{\eta \xi}\right)
    \end{align*}    
\end{enumerate}
\subsection{Винеровский процесс}
\begin{Def}
    Cлучайный процесс $\{W(t), \ t \geq 0\}$ называется \textbf{винеровским
      (процессом броуновского движения)}, если
    \begin{enumerate}
        \item $W(0) = 0$ п.~н.
        \item это процесс с независимыми приращениями
        \item $\forall t, s \geq 0 \ W(t) - W(s) \in \cN(0, \left| t-s \right|)$
    \end{enumerate}
\end{Def}
\begin{Def}
    Cлучайный процесс $\{W(t), \ t \geq 0\}$ называется \textbf{винеровским с
      параметром $\sigma^2$}, если его определение отличается от винеровского
    лишь третьим пунктом:
    \\
    $\forall t, s \geq 0 \ W(t) - W(s) \in \cN(0, \sigma^2 \left| t-s \right|)$
\end{Def}
\begin{theorem}
    Винеровский процесс является нормальным.
\end{theorem}
\begin{Proof}
    Пусть $\forall n \geq 1$ рассмотрим числа $t_1\leq t_2\leq \dots \leq t_n
    \in \RR_+$.
     \begin{align*}
      & X = \left(W(t_1), \dots, W(t_n)\right); \ W(t) = W(t) - W(0) \in \cN(0, t)
     \end{align*}
      \begin{align*}
      & W(t_j) - W(t_{j-1}) \in \cN(0, t_j-t_{j-1})
      \end{align*}
      Они независимы в совокупности, и вектор
      \begin{align*}
        Y = \left[ \begin{matrix}
                W(t_1) - W(0) \\
                W(t_2) - W(t_1) \\
                \dots \\
                W(t_n) - W(t_{n-1})
            \end{matrix} \right]
      \end{align*} 
    \begin{align*}
      & X =\left[ \begin{matrix}
              1 & 0 & \dots & 0 \\
              1 & 1 & \dots & 0 \\
              \dots & \dots & \dots & \dots \\
              1 & 1 & \dots & 1
          \end{matrix} \right] Y
    \end{align*}
    Тогда это нормальный вектор.
    \\
    Если неупорядочены $t_i$~--- то можно упорядочить это линейным преобразованием.
\end{Proof}
Пуассоновский, винеровский и другие процессы называются \textbf{процессами
  Леви}~--- процессами со стационарными независимыми приращениями. Всякий такой
процесс есть композиция нормального и сложного пуассоновского.
\begin{Prop}
    Почти все траектории винеровского процесса всюду непрерывны, но нигде не дифференцируемы.
\end{Prop}
\begin{Prop}
    \begin{align*}
      & \EE W(t) = 0, \ R_W(t,s) = \min(t,s)
    \end{align*}
\end{Prop}
\begin{theorem} Вика (без доказательства)
    \\
    Пусть
    \begin{align*}
      & (X_1, X_2, \dots, X_n) \in \cN (0, R)
    \end{align*}
    Тогда если $n$ нечетно, то
    \begin{align*}
      & \EE X_1X_2\dots X_n = 0
    \end{align*}
    Если же $n$ четно, тогда
    \begin{align*}
      & \EE X_1X_2\dots X_n = \sum R_{p_1q_1}\dots R_{p_{\frac{n}{2}}}
    \end{align*}
    где сумма берется по всем неупорядоченным разбиениям $\{1, \dots, n\}$ на
    $\dst \frac{n}{2}$ неупорядоченных пар.
\end{theorem}
\begin{example}
    \begin{align*}
      & X = \left( X_1, \dots, X_n \right) \in \cN(0, R)
    \end{align*}
    \begin{align*}
      & \EE X_1 = 0, \ \EE X_1X_2X_3 = 0, \ \EE X_1^3 = 0, \ \EE X_1^2X_2^3 = 0
    \end{align*}
    \begin{align*}
      & \EE X_1X_2X_3X_4 = R_{12}R_{34} +R_{13}R_{24} + R_{14}R_{23}
    \end{align*}
    \begin{align*}
      & 12 \mid 34 \sim 34 \mid 12, \ 12 \mid 34 \sim 21 \mid 43
    \end{align*}
\end{example}
\begin{example}
    \begin{align*}
      & \EE W^4(t)
    \end{align*}
    \begin{align*}
      & X = \left( X_1, X_2, X_3, X_4 \right)^T = \left( W(t), W(t), W(t), W(t) \right)^T
    \end{align*}
    \begin{align*}
      & \EE W^4(t) = \EE X_1X_2X_3X_4 = R_{12}R_{34} +R_{13}R_{24} + R_{14}R_{23} = 3 \cov(W(t),W(t))^2 = 3t^2
    \end{align*}
\end{example}
\subsection{Случайные блуждания и винеровский процесс}
\begin{Def}
    Пусть
    \begin{align*}
      & S_n = \sum_{i=1}^n \xi_i
    \end{align*}
    где $\{\xi_i\}$ независимы в совокупности. Такие процессы называются
    \textbf{случайными блужданиями.}
\end{Def}
\begin{theorem} ~
    \\
    Пусть
    \begin{align*}
      & \{\xi_i\} \ \IID, \ \EE \xi_i = 0, \ \DD \xi_i = \sigma^2
    \end{align*}
    Тогда конечномерные распределения процесса
    \begin{align*}
      & X_n (t) = \frac{1}{\sigma \sqrt{n}} \sum_{k=1}^{\left[ nt \right]} \xi_k
    \end{align*}
    сходятся к конечномерным распределениям винеровского процесса.
    \\
    Считаем, что $X_n(t) = 0$ при $[nt]=0$.
\end{theorem}
\begin{Proof}
    Заметим, что
    \begin{align*}
      & \frac{1}{\sigma \sqrt{n}}S_{[nt]} \us{n \to \infty}{\os{d}{\to}} W(t)
    \end{align*}
    Рассмотрим $s<t$.
    \begin{align*}
      & \left( X_n(t), X_n(s) \right) \us{n \to \infty}{\os{d}{\to}} (W(s), W(t))
    \end{align*}
    \begin{align*}
      & \left( X_n(t), X_n(t) - X_n(s) \right) \us{n \to \infty}{\os{d}{\to}} (W(s), W(t)-W(s))
    \end{align*}
    Утверждения равносильны, поскольку из теории вероятностей известно:
    \begin{align*}
      & \left( \xi_n, \eta_n \right) \us{n \to \infty}{\os{d}{\to}} (\xi, \eta) \Leftrightarrow \left( \xi_n, \xi_n + \eta_n \right) \us{n \to \infty}{\os{d}{\to}} (\xi, \xi+\eta)
    \end{align*}
    Второе утверждение:
    \begin{align*}
      & \left( \frac{1}{\sigma \sqrt{n}}S_{[ns]}, \frac{1}{\sigma \sqrt{n}}S_{[nt]} - \frac{1}{\sigma \sqrt{n}}S_{[ns]} \right) \us{n \to \infty}{\os{d}{\to}} \left( W(s), W(t)-W(s) \right)
    \end{align*}
    Имеем право так делать, поскольку правая и левая части независимы.
    \\
    Аналогично, выражая все через приращения, сможем выразить все многомерные распределения.
\end{Proof}